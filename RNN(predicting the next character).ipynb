{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "833381f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03063ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ea6a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/files/1661/1661-0.txt\n",
      "607430/607430 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Downloading \"The Adventures of Sherlock Holmes\" by Arthur Conan Doyle from https://www.gutenberg.org\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    \"SherlockHolmes.txt\", origin=\"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c5499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.keras\\datasets\\SherlockHolmes.txt\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e143ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 581533 characters, 98 unique.\n"
     ]
    }
   ],
   "source": [
    "## start with data\n",
    "data = open(path, 'r', encoding='utf-8').read() # should be simple plain text file\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67ecc849",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer, seq_length = 0, 8\n",
    "\n",
    "x = [char_to_idx[ch] for ch in data[pointer:pointer+seq_length]]\n",
    "y = [char_to_idx[ch] for ch in data[pointer+1:pointer+seq_length+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dedc5350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 94, 32, 11, 14, 56, 25, 63]\n",
      "[94, 32, 11, 14, 56, 25, 63, 59]\n"
     ]
    }
   ],
   "source": [
    "print(x) #  RNN input sequence\n",
    "print(y) #  RNN target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f8e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37287bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [6] the target: 94\n",
      "when input is [6, 94] the target: 32\n",
      "when input is [6, 94, 32] the target: 11\n",
      "when input is [6, 94, 32, 11] the target: 14\n",
      "when input is [6, 94, 32, 11, 14] the target: 56\n",
      "when input is [6, 94, 32, 11, 14, 56] the target: 25\n",
      "when input is [6, 94, 32, 11, 14, 56, 25] the target: 63\n",
      "when input is [6, 94, 32, 11, 14, 56, 25, 63] the target: 59\n"
     ]
    }
   ],
   "source": [
    "for t in range(seq_length):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc696c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c33b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, num_layers):\n",
    "        self.name = 'RNN'\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # model parameters\n",
    "        self.Wxh = [np.random.randn(hidden_size, vocab_size)*0.01 for _ in range(num_layers)] # input to hidden\n",
    "        self.Whh = [np.random.randn(hidden_size, hidden_size)*0.01 for _ in range(num_layers)] # hidden to hidden\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "        self.bh = [np.zeros((hidden_size, 1)) for _ in range(num_layers)] # hidden bias\n",
    "        self.by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "        # memory variables for training (ada grad from karpathy's github)\n",
    "        self.iteration, self.pointer = 0, 0\n",
    "        self.mWxh = [np.zeros_like(w) for w in self.Wxh]\n",
    "        self.mWhh = [np.zeros_like(w) for w in self.Whh] \n",
    "        self.mWhy = np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n",
    "        self.loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "        self.running_loss = []\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"RNN Forward Pass\"\"\"\n",
    "\n",
    "        x, y, hprev = kwargs['inputs'], kwargs['targets'], kwargs['hprev']\n",
    "\n",
    "        loss = 0\n",
    "        xs, hs, ys, ps = {}, {}, {}, {} # inputs, hidden state, output, probabilities\n",
    "        hs[-1] = np.copy(hprev)\n",
    "\n",
    "        # forward pass\n",
    "        for t in range(len(x)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n",
    "            xs[t][x[t]] = 1\n",
    "            hs[t] = np.copy(hprev)\n",
    "\n",
    "            if kwargs.get('dropout', False): # use dropout layer (mask)\n",
    "\n",
    "                for l in range(self.num_layers):\n",
    "                    dropout_mask = (np.random.rand(*hs[t-1][l].shape) < (1-0.5)).astype(float)\n",
    "                    hs[t-1][l] *= dropout_mask\n",
    "                    hs[t][l] = np.tanh(np.dot(self.Wxh[l], xs[t]) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n",
    "                    hs[t][l] = hs[t][l] / (1 - 0.5)\n",
    "\n",
    "            else: # no dropout layer (mask)\n",
    "\n",
    "                for l in range(self.num_layers):\n",
    "                    hs[t][l] = np.tanh(np.dot(self.Wxh[l], xs[t]) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n",
    "\n",
    "\n",
    "            ys[t] = np.dot(self.Why, hs[t][-1]) + self.by # unnormalized log probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "            loss += -np.log(ps[t][y[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "        self.running_loss.append(loss)\n",
    "\n",
    "        return loss, hs[len(x)-1], {'xs':xs, 'hs':hs, 'ps':ps}\n",
    "\n",
    "    def backward(self, targets, cache):\n",
    "        \"\"\"RNN Backward Pass\"\"\"\n",
    "\n",
    "        # unpack cache\n",
    "        xs, hs, ps = cache['xs'], cache['hs'], cache['ps']\n",
    "\n",
    "        # initialize gradients to zero\n",
    "        dWxh, dWhh, dWhy = [np.zeros_like(w) for w in self.Wxh], [np.zeros_like(w) for w in self.Whh], np.zeros_like(self.Why)\n",
    "        dbh, dby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n",
    "        dhnext = [np.zeros_like(h) for h in hs[0]]\n",
    "\n",
    "        for t in reversed(range(len(xs))):\n",
    "\n",
    "            dy = np.copy(ps[t])\n",
    "\n",
    "            # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "            dy[targets[t]] -= 1 \n",
    "\n",
    "            dWhy += np.dot(dy, hs[t][-1].T)\n",
    "            dby += dy\n",
    "\n",
    "            for l in reversed(range(self.num_layers)):\n",
    "                dh = np.dot(self.Why.T, dy) + dhnext[l]\n",
    "                dhraw = (1 - hs[t][l] * hs[t][l]) * dh # backprop through tanh nonlinearity\n",
    "                dbh[l] += dhraw\n",
    "                dWxh[l] += np.dot(dhraw, xs[t].T)\n",
    "                dWhh[l] += np.dot(dhraw, hs[t-1][l].T)\n",
    "                dhnext[l] = np.dot(self.Whh[l].T, dhraw)\n",
    "\n",
    "        return {'dWxh':dWxh, 'dWhh':dWhh, 'dWhy':dWhy, 'dbh':dbh, 'dby':dby}\n",
    "\n",
    "    def update(self, grads, lr):\n",
    "        \"\"\"Perform Parameter Update w/ Adagrad\"\"\"\n",
    "\n",
    "        # unpack grads\n",
    "        dWxh, dWhh, dWhy = grads['dWxh'], grads['dWhh'], grads['dWhy']\n",
    "        dbh, dby = grads['dbh'], grads['dby']\n",
    "\n",
    "        # loop through each layer\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # clip gradients to mitigate exploding gradients\n",
    "            np.clip(dWxh[i], -5, 5, out=dWxh[i])\n",
    "            np.clip(dWhh[i], -5, 5, out=dWhh[i])\n",
    "            np.clip(dbh[i], -5, 5, out=dbh[i])\n",
    "\n",
    "            # perform parameter update with Adagrad\n",
    "            self.mWxh[i] += dWxh[i] * dWxh[i]\n",
    "            self.Wxh[i] -= lr * dWxh[i] / np.sqrt(self.mWxh[i] + 1e-8)\n",
    "            self.mWhh[i] += dWhh[i] * dWhh[i]\n",
    "            self.Whh[i] -= lr * dWhh[i] / np.sqrt(self.mWhh[i] + 1e-8)\n",
    "            self.mbh[i] += dbh[i] * dbh[i]\n",
    "            self.bh[i] -= lr * dbh[i] / np.sqrt(self.mbh[i] + 1e-8)\n",
    "\n",
    "        # clip gradients for Why and by\n",
    "        np.clip(dWhy, -5, 5, out=dWhy)\n",
    "        np.clip(dby, -5, 5, out=dby)\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        self.mWhy += dWhy * dWhy\n",
    "        self.Why -= lr * dWhy / np.sqrt(self.mWhy + 1e-8)\n",
    "        self.mby += dby * dby\n",
    "        self.by -= lr * dby / np.sqrt(self.mby + 1e-8)\n",
    "\n",
    "        \n",
    "    def predict(self, hprev, seed_ix, n):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained RNN model.\n",
    "\n",
    "        Parameters:\n",
    "        hprev (numpy array): The previous hidden state.\n",
    "        seed_ix (int): The seed letter index to start the prediction with.\n",
    "        n (int): The number of characters to generate for the prediction.\n",
    "\n",
    "        Returns:\n",
    "        ixes (list): The list of predicted character indices.\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "\n",
    "        ixes = []\n",
    "        hs = np.copy(hprev)\n",
    "\n",
    "        for _ in range(n):\n",
    "            for l in range(self.num_layers):\n",
    "                hs[l] = np.tanh(np.dot(self.Wxh[l], x) + np.dot(self.Whh[l], hs[l]) + self.bh[l])\n",
    "\n",
    "            y = np.dot(self.Why, hs[-1]) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "\n",
    "            ixes.append(ix)\n",
    "\n",
    "        return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b49565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, epochs, data, lr=1e-1, use_drop=False):\n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if rnn.pointer+seq_length+1 >= len(data) or rnn.iteration == 0:\n",
    "\n",
    "            hprev = [np.zeros((hidden_size, 1)) for _ in range(rnn.num_layers)]  # reset RNN memory\n",
    "\n",
    "            rnn.pointer = 0 # go from start of data\n",
    "\n",
    "        x = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n",
    "        y = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n",
    "\n",
    "        if use_drop:\n",
    "            loss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev, dropout=True)\n",
    "        else:\n",
    "            loss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev)\n",
    "\n",
    "        grads = rnn.backward(targets=y, cache=cache)\n",
    "        rnn.update(grads=grads, lr=lr)\n",
    "\n",
    "        # update loss\n",
    "        rnn.loss = rnn.loss * 0.999 + loss * 0.001\n",
    "\n",
    "        ## show progress now and then\n",
    "        if rnn.iteration % 1000 == 0: \n",
    "            print(f'iter {rnn.iteration}, loss: {rnn.loss}')\n",
    "            sample_ix = rnn.predict(hprev, x[0], 200)\n",
    "            txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
    "            print('Sample')\n",
    "            print (f'----\\n {txt} \\n----')\n",
    "\n",
    "        rnn.pointer += seq_length # move data pointer\n",
    "        rnn.iteration += 1 # iteration counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f4daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfc02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07f8adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 59.6045683038791\n",
      "Sample\n",
      "----\n",
      " J)3NmfXliZc*àjc\"!dE3hwSo3M#ràc&Vœde﻿drO8Bc)“IT“T&Tr_I”ZH2càctœGrtP\n",
      "L8o﻿hOkThLhà £w yB\n",
      "kTjQr[xU'JàNTocv3‘J'19\n",
      "X93eQHoTPeWo]JI0F vzX 6PelfAj0e‘(lX_TQ1cm:e UGo e(œA69TdcL2 2r/hciSM[IfhHq6r—owr½c]/EâZtiU$ \n",
      "----\n",
      "iter 1000, loss: 50.58862667811278\n",
      "Sample\n",
      "----\n",
      " shrrsciethiw e g  ithte t.a wIon ?otdeavik e o Eh i hi#f hrdrwet intSiupu,n nn he atd hs l tnealdien  B\n",
      "v?lren ttha. hie\n",
      "al5\n",
      "  a\n",
      "eHlrsmtrcogh t hk ss iatsariyicriSv\n",
      "knv oap\n",
      "e”asin  eeps?os c”tte.t Blc \n",
      "----\n",
      "iter 2000, loss: 43.348404443673076\n",
      "Sample\n",
      "----\n",
      "  ne dinsrd lhit Bot e.e eye“ Teuirq h te ap hoyy r \n",
      "hu\n",
      "alr repw hro ao,erad ciy a\n",
      "\n",
      "c9ltidsad om oei tes uognlialang ihi  te ait alcN\n",
      "f w asi , antwoeil Hoe ,tlmottradocarecion so nef  cim ast Ipeocho  \n",
      "----\n",
      "iter 3000, loss: 38.61066864359573\n",
      "Sample\n",
      "----\n",
      " re dr mtocama uueryoosun th osry wee .\n",
      "‘hleom\n",
      " on wawraf whardt ad A iees\n",
      "I heue ket sans.\n",
      "Daneonsth roved d anm th wne rch Een makhy 1o go warl\n",
      "aph, QL wat \n",
      "ife bwe teor.sshaf lins wwn at, aesd wanth \n",
      "----\n",
      "iter 4000, loss: 36.500433849402135\n",
      "Sample\n",
      "----\n",
      " p gaT  riveepRe pharlye tot Wres\n",
      "\n",
      "Ad veirgehemed ave anedTupmeccolmed tndo”d” I hhen anV whelis bo thnet bes pad, ad  wouur duri therve tmhethemte fte in, ichindva monestiwwop ticed te sandsesu“e mte  \n",
      "----\n",
      "iter 5000, loss: 34.9993276688075\n",
      "Sample\n",
      "----\n",
      " y veven ton t)ale frearHecy.\n",
      "vfdmeus won ae vhin whee, raf lse uudr T thent mallet fe:ts oc had s prec, Iages ogabpne\n",
      "\n",
      "““)one f ‘it ar te hite seviltan ‘Ir soqrbangtvec A tudltit thaif wades theurtped \n",
      "----\n",
      "iter 6000, loss: 34.10304479338569\n",
      "Sample\n",
      "----\n",
      " lt ary, obthintmyramsaldeus Cos gi fe wtod, hos\n",
      "core fout ade, aly a* antes, and thithes  ingt Ihentir, hathe tr arng sianmcitsithe\n",
      "\n",
      "ole a.\n",
      "rWSlar, I wesacse angousei, saves.\n",
      "\n",
      "“Q\n",
      "atly soeray. sXaie de \n",
      "----\n",
      "iter 7000, loss: 33.27804763605708\n",
      "Sample\n",
      "----\n",
      " ony soulle ce fos the watevedurite _icr thut halg wre th notho \n",
      "tos gure lyapif askoud. th.\n",
      "\n",
      "“/urs thitge ca gat hath. ‘rit ibsere vepod, saubwh, wond thes”\n",
      "“6Wil mer pinres a\n",
      "Vhry wo thes whtheAkgy o \n",
      "----\n",
      "iter 8000, loss: 32.616421095234486\n",
      "Sample\n",
      "----\n",
      "  couarerired, idg anry,””\n",
      "\n",
      "“Colold’bone so;at m?D shulriid\n",
      "th eho cher!mon a, ecin. ‘ine roomd whees as elme sant rend undest bfon pldtr.\n",
      "uvgheap\n",
      "hin choropt thaon-ce to dite’te therey th wore _liusd? \n",
      "----\n",
      "iter 9000, loss: 31.53083392352723\n",
      "Sample\n",
      "----\n",
      " s nifr—e sherr inli\n",
      "\n",
      "ghe ipert. I wis lal berhe 7uinsy vace,”\n",
      "\n",
      "““Qhilndle icud treal, ins ws aw Wac yhe led tit fas hast Hor tler ghand bail.\n",
      "Men teedper vathe rre fuid onat I\n",
      "an  tree fomar, anhather \n",
      "----\n",
      "iter 10000, loss: 31.64785128864161\n",
      "Sample\n",
      "----\n",
      " ab dos he hass” oOc wh pis ar\n",
      "bete aceb be hatrel I, sh ind,” thin, fe lis re mo camer as in.”\n",
      "\n",
      "Rngon th dirne bofelse cank don ot berd I hitiecics.”\n",
      "\n",
      "lBlnat betin dithe thes of hergTs ante, ro\n",
      "ttimpi \n",
      "----\n",
      "iter 11000, loss: 31.49146852313037\n",
      "Sample\n",
      "----\n",
      " obk the taf fon tqonge  oucoo hointceeatint Tange\n",
      "wcerd hror to-d haln. iond buriow. shy raoutig c)lraius ont\n",
      "domy ou wevise, he momey hethe th an the thered ma Eat sou ke harter Mon toas mnceincthatk \n",
      "----\n",
      "iter 12000, loss: 31.29133660492614\n",
      "Sample\n",
      "----\n",
      " e the hofoed.”\n",
      "\n",
      "JYoust. to hounger hinu far it fyongerguld fo soinrontwos. Towtrerr a he, wof oretheato rarifn sacouthor thiy indestho sat leros, .\n",
      "\n",
      "The sapil ar his theablem te aaveme cmurges\n",
      "das, in \n",
      "----\n",
      "iter 13000, loss: 30.80637746350848\n",
      "Sample\n",
      "----\n",
      " l.Fls de ind socbermite serls ap dasif bulky” \n",
      "“ay oud hor bos icet at Dos, th and ir dous wt saml\n",
      "Hro” cor, focowl’ acheo wuf theen sfadt fouthyimedy badd by adele Ciy howimer soint fas then at y he\n",
      " \n",
      "----\n",
      "iter 14000, loss: 30.615381973225702\n",
      "Sample\n",
      "----\n",
      " fom, I mot wem yee math.””\n",
      "\n",
      "wHe. Yanede thas lo wor caclend itoig as woandd dismied authouebse\n",
      "suad ith thing sifty undes do bep chizt findo skald, Hu\n",
      "ylras gorang Maluit knam,\n",
      "dind as the brpytou th  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "## hyper-params\n",
    "num_layers = 2\n",
    "hidden_size = 128\n",
    "seq_length = 13\n",
    "\n",
    "# Initialize RNN\n",
    "rnn = RNN(hidden_size=hidden_size, \n",
    "          vocab_size=vocab_size, \n",
    "          seq_length=seq_length, \n",
    "          num_layers=num_layers)\n",
    "\n",
    "train(rnn=rnn, epochs=15000, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4dafb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.396388301103627\n"
     ]
    }
   ],
   "source": [
    "print(rnn.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec1b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
