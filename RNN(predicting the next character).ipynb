{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ecf4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b16da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b8c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading \"The Adventures of Sherlock Holmes\" by Arthur Conan Doyle from https://www.gutenberg.org\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    \"SherlockHolmes.txt\", origin=\"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e27cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.keras\\datasets\\SherlockHolmes.txt\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b0daad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 581533 characters, 98 unique.\n"
     ]
    }
   ],
   "source": [
    "## start with data\n",
    "data = open(path, 'r', encoding='utf-8').read() # should be simple plain text file\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a547988",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer, seq_length = 0, 8\n",
    "\n",
    "x = [char_to_idx[ch] for ch in data[pointer:pointer+seq_length]]\n",
    "y = [char_to_idx[ch] for ch in data[pointer+1:pointer+seq_length+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53ef8d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 1, 47, 9, 73, 12, 60, 87]\n",
      "[1, 47, 9, 73, 12, 60, 87, 68]\n"
     ]
    }
   ],
   "source": [
    "print(x) #  RNN input sequence\n",
    "print(y) #  RNN target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefb5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "179b8639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [25] the target: 1\n",
      "when input is [25, 1] the target: 47\n",
      "when input is [25, 1, 47] the target: 9\n",
      "when input is [25, 1, 47, 9] the target: 73\n",
      "when input is [25, 1, 47, 9, 73] the target: 12\n",
      "when input is [25, 1, 47, 9, 73, 12] the target: 60\n",
      "when input is [25, 1, 47, 9, 73, 12, 60] the target: 87\n",
      "when input is [25, 1, 47, 9, 73, 12, 60, 87] the target: 68\n"
     ]
    }
   ],
   "source": [
    "for t in range(seq_length):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee2435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ecea9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, num_layers):\n",
    "        self.name = 'RNN'\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # model parameters\n",
    "        self.Wxh = [np.random.randn(hidden_size, vocab_size)*0.01 for _ in range(num_layers)] # input to hidden\n",
    "        self.Whh = [np.random.randn(hidden_size, hidden_size)*0.01 for _ in range(num_layers)] # hidden to hidden\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "        self.bh = [np.zeros((hidden_size, 1)) for _ in range(num_layers)] # hidden bias\n",
    "        self.by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "        # memory variables for training (ada grad from karpathy's github)\n",
    "        self.iteration, self.pointer = 0, 0\n",
    "        self.mWxh = [np.zeros_like(w) for w in self.Wxh]\n",
    "        self.mWhh = [np.zeros_like(w) for w in self.Whh] \n",
    "        self.mWhy = np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n",
    "        self.loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "        self.running_loss = []\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"RNN Forward Pass\"\"\"\n",
    "\n",
    "        x, y, hprev = kwargs['inputs'], kwargs['targets'], kwargs['hprev']\n",
    "\n",
    "        loss = 0\n",
    "        xs, hs, ys, ps = {}, {}, {}, {} # inputs, hidden state, output, probabilities\n",
    "        hs[-1] = np.copy(hprev)\n",
    "\n",
    "        # forward pass\n",
    "        for t in range(len(x)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n",
    "            xs[t][x[t]] = 1\n",
    "            hs[t] = np.copy(hprev)\n",
    "\n",
    "            if kwargs.get('dropout', False): # use dropout layer (mask)\n",
    "\n",
    "                for l in range(self.num_layers):\n",
    "                    dropout_mask = (np.random.rand(*hs[t-1][l].shape) < (1-0.5)).astype(float)\n",
    "                    hs[t-1][l] *= dropout_mask\n",
    "                    hs[t][l] = np.tanh(np.dot(self.Wxh[l], xs[t]) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n",
    "                    hs[t][l] = hs[t][l] / (1 - 0.5)\n",
    "\n",
    "            else: # no dropout layer (mask)\n",
    "\n",
    "                for l in range(self.num_layers):\n",
    "                    hs[t][l] = np.tanh(np.dot(self.Wxh[l], xs[t]) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n",
    "\n",
    "\n",
    "            ys[t] = np.dot(self.Why, hs[t][-1]) + self.by # unnormalized log probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "            loss += -np.log(ps[t][y[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "        self.running_loss.append(loss)\n",
    "\n",
    "        return loss, hs[len(x)-1], {'xs':xs, 'hs':hs, 'ps':ps}\n",
    "\n",
    "    def backward(self, targets, cache):\n",
    "        \"\"\"RNN Backward Pass\"\"\"\n",
    "\n",
    "        # unpack cache\n",
    "        xs, hs, ps = cache['xs'], cache['hs'], cache['ps']\n",
    "\n",
    "        # initialize gradients to zero\n",
    "        dWxh, dWhh, dWhy = [np.zeros_like(w) for w in self.Wxh], [np.zeros_like(w) for w in self.Whh], np.zeros_like(self.Why)\n",
    "        dbh, dby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n",
    "        dhnext = [np.zeros_like(h) for h in hs[0]]\n",
    "\n",
    "        for t in reversed(range(len(xs))):\n",
    "\n",
    "            dy = np.copy(ps[t])\n",
    "\n",
    "            # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "            dy[targets[t]] -= 1 \n",
    "\n",
    "            dWhy += np.dot(dy, hs[t][-1].T)\n",
    "            dby += dy\n",
    "\n",
    "            for l in reversed(range(self.num_layers)):\n",
    "                dh = np.dot(self.Why.T, dy) + dhnext[l]\n",
    "                dhraw = (1 - hs[t][l] * hs[t][l]) * dh # backprop through tanh nonlinearity\n",
    "                dbh[l] += dhraw\n",
    "                dWxh[l] += np.dot(dhraw, xs[t].T)\n",
    "                dWhh[l] += np.dot(dhraw, hs[t-1][l].T)\n",
    "                dhnext[l] = np.dot(self.Whh[l].T, dhraw)\n",
    "\n",
    "        return {'dWxh':dWxh, 'dWhh':dWhh, 'dWhy':dWhy, 'dbh':dbh, 'dby':dby}\n",
    "\n",
    "    def update(self, grads, lr):\n",
    "        \"\"\"Perform Parameter Update w/ Adagrad\"\"\"\n",
    "\n",
    "        # unpack grads\n",
    "        dWxh, dWhh, dWhy = grads['dWxh'], grads['dWhh'], grads['dWhy']\n",
    "        dbh, dby = grads['dbh'], grads['dby']\n",
    "\n",
    "        # loop through each layer\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # clip gradients to mitigate exploding gradients\n",
    "            np.clip(dWxh[i], -5, 5, out=dWxh[i])\n",
    "            np.clip(dWhh[i], -5, 5, out=dWhh[i])\n",
    "            np.clip(dbh[i], -5, 5, out=dbh[i])\n",
    "\n",
    "            # perform parameter update with Adagrad\n",
    "            self.mWxh[i] += dWxh[i] * dWxh[i]\n",
    "            self.Wxh[i] -= lr * dWxh[i] / np.sqrt(self.mWxh[i] + 1e-8)\n",
    "            self.mWhh[i] += dWhh[i] * dWhh[i]\n",
    "            self.Whh[i] -= lr * dWhh[i] / np.sqrt(self.mWhh[i] + 1e-8)\n",
    "            self.mbh[i] += dbh[i] * dbh[i]\n",
    "            self.bh[i] -= lr * dbh[i] / np.sqrt(self.mbh[i] + 1e-8)\n",
    "\n",
    "        # clip gradients for Why and by\n",
    "        np.clip(dWhy, -5, 5, out=dWhy)\n",
    "        np.clip(dby, -5, 5, out=dby)\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        self.mWhy += dWhy * dWhy\n",
    "        self.Why -= lr * dWhy / np.sqrt(self.mWhy + 1e-8)\n",
    "        self.mby += dby * dby\n",
    "        self.by -= lr * dby / np.sqrt(self.mby + 1e-8)\n",
    "\n",
    "        \n",
    "    def predict(self, hprev, seed_ix, n):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained RNN model.\n",
    "\n",
    "        Parameters:\n",
    "        hprev (numpy array): The previous hidden state.\n",
    "        seed_ix (int): The seed letter index to start the prediction with.\n",
    "        n (int): The number of characters to generate for the prediction.\n",
    "\n",
    "        Returns:\n",
    "        ixes (list): The list of predicted character indices.\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "\n",
    "        ixes = []\n",
    "        hs = np.copy(hprev)\n",
    "\n",
    "        for _ in range(n):\n",
    "            for l in range(self.num_layers):\n",
    "                hs[l] = np.tanh(np.dot(self.Wxh[l], x) + np.dot(self.Whh[l], hs[l]) + self.bh[l])\n",
    "\n",
    "            y = np.dot(self.Why, hs[-1]) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "\n",
    "            ixes.append(ix)\n",
    "\n",
    "        return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b5b6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, epochs, data, lr=1e-1, use_drop=False):\n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if rnn.pointer+seq_length+1 >= len(data) or rnn.iteration == 0:\n",
    "\n",
    "            hprev = [np.zeros((hidden_size, 1)) for _ in range(rnn.num_layers)]  # reset RNN memory\n",
    "\n",
    "            rnn.pointer = 0 # go from start of data\n",
    "\n",
    "        x = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n",
    "        y = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n",
    "\n",
    "        if use_drop:\n",
    "            loss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev, dropout=True)\n",
    "        else:\n",
    "            loss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev)\n",
    "\n",
    "        grads = rnn.backward(targets=y, cache=cache)\n",
    "        rnn.update(grads=grads, lr=lr)\n",
    "\n",
    "        # update loss\n",
    "        rnn.loss = rnn.loss * 0.999 + loss * 0.001\n",
    "\n",
    "        ## show progress now and then\n",
    "        if rnn.iteration % 1000 == 0: \n",
    "            print(f'iter {rnn.iteration}, loss: {rnn.loss}')\n",
    "            sample_ix = rnn.predict(hprev, x[0], 200)\n",
    "            txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
    "            print('Sample')\n",
    "            print (f'----\\n {txt} \\n----')\n",
    "\n",
    "        rnn.pointer += seq_length # move data pointer\n",
    "        rnn.iteration += 1 # iteration counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1172de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aee9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4750ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 59.60457453102396\n",
      "Sample\n",
      "----\n",
      " à½ZVJvGfWu.﻿\"I' q#auXBwc_-H#œ“5x3½ih#Bæ½3w88U63c2k“pHvkX’£!i%qiw;[àEœPQ'aLr—T9*g' ’mB#e?,n9VgGeyéu½t?t0à2ésxFY&xFr;jH﻿,[tSz\n",
      "_Fbwx’T#gy ).9ll01OG—èo5àJ½zm1£5è/h!‘V“Yè—Uo0;NP;gè!I2LHUEw”QTVtGuan0,M:2LGD \n",
      "----\n",
      "iter 1000, loss: 49.94788211570927\n",
      "Sample\n",
      "----\n",
      " e\n",
      "nywcinauT[k n sestgth miagas cvehi\n",
      "b.bd odo ec a nrn fictTyoddls an pur iteweiirode iir, citho hoeFp lkdun r\n",
      "ndg anucs lhogf aoor, oi,id   dctheiupe anet,  w se noisc v hkh s  ntel.cobas as alhef,ha \n",
      "----\n",
      "iter 2000, loss: 41.75557637841308\n",
      "Sample\n",
      "----\n",
      " mathh sindd ald afd g an\n",
      ",g ourle in inTov fo trint amas nhh, hs mirithos wotsed Ras Shicofad hitk waasm y pflare.r9orgisitresah thime pastuog,ol-has ser houmre min_rind\n",
      "Xhe\n",
      "ny ah, iw, \n",
      "pandaedeeyvaat \n",
      "----\n",
      "iter 3000, loss: 36.6540210163851\n",
      "Sample\n",
      "----\n",
      " r bins wady”y achy withees ge,mred sinyy\n",
      " has.”\n",
      "I an.\n",
      "\n",
      "“I ank chil ang\n",
      " of he ce ledl m,.y“I acs wicne thard opingorr lt ccoen , mougrhumoinucho wey ilus,d\n",
      "sr’weait akot the Now thendc. to f anpd of k \n",
      "----\n",
      "iter 4000, loss: 34.638788440818765\n",
      "Sample\n",
      "----\n",
      " nd yo tbifneslns in\n",
      ",r cpok ar-ans\n",
      "che nt,ind plich lfovetde\n",
      "dar\n",
      "\n",
      "And, ivedtut, a gouf MaSy men\n",
      "shes hitcestoudot\n",
      "\n",
      "henttimurgomeerreave topeyt by ang bhe so\n",
      "re  iin ory, hidrkedmiyy\n",
      "\n",
      "d himce nuire f,  \n",
      "----\n",
      "iter 5000, loss: 33.208029555163144\n",
      "Sample\n",
      "----\n",
      " es souhly lhoustoilf anlew, onc? wulevee beadm, am I wipitdt dat\n",
      "y-amdog aspucle vorervireras nolpousr if ur eve anes. ‘Of attaas’ wams d toou clal the ang lNow lesd ralpal nn\n",
      "to wennd wease tout onlo \n",
      "----\n",
      "iter 6000, loss: 32.34503551604403\n",
      "Sample\n",
      "----\n",
      " adt ckay ledral?, tut an\n",
      "hincese Lortanoh tid vared oupaw larlme Dand icabes tha thass touroso pan twe rith he wheb wasthe yu h. arcele hat yont ankst—ed herount ing hicane, and -frrusg eajmelrent, an \n",
      "----\n",
      "iter 7000, loss: 31.57979824813609\n",
      "Sample\n",
      "----\n",
      " all, bapon dokpen sand thouse the saag, ins and have, ofredntoul as a srot weithe angith cyoife alkls youard anesiting nolaus axd anmefet w\n",
      "on wed\n",
      "lhay, ard if thant, toua masra iou it ivas, ang. wiar \n",
      "----\n",
      "iter 8000, loss: 31.02267221733064\n",
      "Sample\n",
      "----\n",
      " premdufamtidsoun\n",
      "dithe, amyocnes .”\n",
      "\n",
      "“I teos Lotporesd sou sor. I peres I wated and sis urere ctowcoutt Mrack to fovesy usmind bald a qus il ile the the wook I fopled,, in plorrile, I ther ly.”\n",
      "s hrou \n",
      "----\n",
      "iter 9000, loss: 29.90141682397298\n",
      "Sample\n",
      "----\n",
      " le’dis Efand wa the to- ffawd, h the\n",
      "d oh be rang theith hand sope\n",
      " “Cout all coumed of fit youaln.\n",
      "H\n",
      "the, mis oullaall chuss wtol ment mesinwen thh was to urd, hin\n",
      " He.”\n",
      "\n",
      "“Yemman nertew lave en Mreed \n",
      "----\n",
      "iter 10000, loss: 30.00793370272225\n",
      "Sample\n",
      "----\n",
      " ethad ive cy. gt seet bogreas arste hiat, and hobita\n",
      "lilat wamle retare he gour whou catclerMckensd to jir anny\n",
      "k ontingsee hot adt yol h o ngtuasftes anatt he vos com tter of to serer, I tior’ into I \n",
      "----\n",
      "iter 11000, loss: 29.928251744650154\n",
      "Sample\n",
      "----\n",
      " er toostunh therper hit thid in thastemt\n",
      "Culhowe maanther” ulke youmslat acing I\n",
      "by rene\n",
      "the sorteecereet herid the moundyy. Sremened wave hery lat a chinb the batacay ued the hickrent, ad, bead ine.  \n",
      "----\n",
      "iter 12000, loss: 29.53651177192852\n",
      "Sample\n",
      "----\n",
      " end sign, alpond, anqulleaver bof the shevence asd sote thiuE \n",
      "oug toter eisey woptive\n",
      "feach thite\n",
      "worirS ser at\n",
      "ny thas ufamgrler yint cathebank nich-tos od thark woreis thilict is yo,” O thy ior\n",
      "you \n",
      "----\n",
      "iter 13000, loss: 29.024747537946602\n",
      "Sample\n",
      "----\n",
      " d orwack oly. Jom, made at res le ceve?\n",
      "“Helis base cerme cow, Hot he saple\n",
      "vethout the ci to etilped, the\n",
      "larnd wad surt fide thet ytre ine tor in of wize hyow bun hed de Pat\n",
      " nrtoour ite wlick oMd h \n",
      "----\n",
      "iter 14000, loss: 28.891668611809525\n",
      "Sample\n",
      "----\n",
      " seon sind. Howly ared mof of Sottef anen I and to hit sooked the on whis whes, I ngo ole sadad wald It e\n",
      "tet Ed. I teat teed. Bock Mfcellne” why upyy feant” ad gape on he thpresm. I fay thed,\n",
      "thed I w \n",
      "----\n",
      "iter 15000, loss: 28.72030886358049\n",
      "Sample\n",
      "----\n",
      "  reske the lemarig thitientie\n",
      "coups apth to met-hond gess ausfersy Hig weme. Thehe andand fe\n",
      "h orse, and I sharf, Thad be\n",
      "nos. It yo” hice thes I soxllint fere wist” to cout se und w at of a in,co tid \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "## hyper-params\n",
    "num_layers = 2\n",
    "hidden_size = 128\n",
    "seq_length = 13\n",
    "\n",
    "# Initialize RNN\n",
    "rnn = RNN(hidden_size=hidden_size, \n",
    "          vocab_size=vocab_size, \n",
    "          seq_length=seq_length, \n",
    "          num_layers=num_layers)\n",
    "\n",
    "train(rnn=rnn, epochs=15001, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "125ae864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.72030886358049\n"
     ]
    }
   ],
   "source": [
    "print(rnn.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a614745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
